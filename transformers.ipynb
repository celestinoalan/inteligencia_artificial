{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVsQwDDua3YEjxuiSgH73F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/celestinoalan/inteligencia_artificial/blob/main/transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers\n",
        "\n",
        "Criamos um transformer e utilizamos para gerar texto similar às obras de Machado de Assis."
      ],
      "metadata": {
        "id": "51vHtUlhdJ6r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oMv8IVlKmdau"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Linear, Module, Dropout, ModuleList, Sequential, LayerNorm, Embedding\n",
        "from torch.nn import functional as F\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dados"
      ],
      "metadata": {
        "id": "35RjCBj6INuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixa machado_de_assis.txt no diretório raíz\n",
        "!wget https://raw.githubusercontent.com/celestinoalan/inteligencia_artificial/main/data/machado_de_assis.txt\n",
        "with open('machado_de_assis.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8W0nTCfHQq_",
        "outputId": "e1b70d33-0833-47d5-f39a-d620c0ae4400"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-24 01:37:45--  https://raw.githubusercontent.com/celestinoalan/inteligencia_artificial/main/data/machado_de_assis.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11340314 (11M) [text/plain]\n",
            "Saving to: ‘machado_de_assis.txt’\n",
            "\n",
            "machado_de_assis.tx 100%[===================>]  10.81M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-10-24 01:37:46 (232 MB/s) - ‘machado_de_assis.txt’ saved [11340314/11340314]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10977697"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, text):\n",
        "        self.chars = sorted(list(set(text)))\n",
        "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
        "\n",
        "    def encode(self, s):\n",
        "        return torch.tensor([self.stoi[c] for c in s], dtype=torch.int)\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        if isinstance(tokens, torch.Tensor):\n",
        "            tokens = tokens.to(\"cpu\").numpy().squeeze()\n",
        "        return ''.join([self.itos[t] for t in tokens])"
      ],
      "metadata": {
        "id": "e5_QnEnPIZi0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "jPTPXiPiImxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = 0.2\n",
        "block_size = 256\n",
        "n_embeds = 384\n",
        "tokenizer = Tokenizer(text)\n",
        "data = tokenizer.encode(text)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "vocab_size = len(tokenizer.stoi)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "class Head(Module):\n",
        "    def __init__(self, head_size: int):\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "        self.query = Linear(n_embeds, self.head_size, bias=False)\n",
        "        self.key = Linear(n_embeds, self.head_size, bias=False)\n",
        "        self.value = Linear(n_embeds, self.head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.query(x)  # (B x T x C) * (C x Hs) = (B x T x Hs)\n",
        "        k = self.key(x)  # (B x T x C) * (C x Hs) = (B x T x Hs)\n",
        "        v = self.value(x)  # (B x T x C) * (C x Hs) = (B x T x Hs)\n",
        "        affinities = q @ k.transpose(-2, -1) / self.head_size ** 0.5  # (B x T x Hs) @ (B x Hs x T) = (B x T x T)\n",
        "        affinities = affinities.masked_fill(self.tril[:affinities.shape[1], :affinities.shape[2]] == 0, float(\"-inf\"))  # For a given block T might be < than block size\n",
        "        affinities = affinities.softmax(dim=-1)\n",
        "        affinities = self.dropout(affinities)\n",
        "        return affinities @ v  # (B x T x T) @ (B x T x Hs) = (B x T x Hs)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(Module):\n",
        "    def __init__(self, num_heads):\n",
        "        super().__init__()\n",
        "        head_size = n_embeds // num_heads\n",
        "        self.heads = ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = Linear(num_heads * head_size, n_embeds)\n",
        "        self.dropout = Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        concat = torch.cat([head(x) for head in self.heads], dim=-1)  # (B x T x Hs*NH)\n",
        "        return self.dropout(self.proj(concat))  # (B x T x Hs*NH) * (Hs*NH x C) = (B x T x C)\n",
        "\n",
        "class FeedForward(Module):\n",
        "    def __init__(self, n_embeds):\n",
        "        super().__init__()\n",
        "        self.net = Sequential(\n",
        "            Linear(n_embeds, 4 * n_embeds),\n",
        "            torch.nn.ReLU(),\n",
        "            Linear(4 * n_embeds, n_embeds),\n",
        "            Dropout(dropout)\n",
        "        )\n",
        "        self.ln1 = Linear(n_embeds, 4 * n_embeds)\n",
        "        self.ln2 = Linear(4 * n_embeds, n_embeds)\n",
        "        self.dropout = Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.ln2(torch.relu(self.ln1(x))))\n",
        "\n",
        "\n",
        "class Block(Module):\n",
        "    def __init__(self, n_embeds, num_heads):\n",
        "        super().__init__()\n",
        "        self.sa = MultiHeadAttention(num_heads)\n",
        "        self.ffwd = FeedForward(n_embeds)\n",
        "        self.ln1 = LayerNorm(n_embeds)\n",
        "        self.ln2 = LayerNorm(n_embeds)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(Module):\n",
        "    def __init__(self, n_blocks: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = Embedding(vocab_size, n_embeds)\n",
        "        self.position_embedding_table = Embedding(block_size, n_embeds)\n",
        "        self.blocks = Sequential(*(Block(n_embeds, num_heads) for _ in range(n_blocks)))\n",
        "        self.ln_f = LayerNorm(n_embeds)\n",
        "        self.lm_head = Linear(n_embeds, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B x T x C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T x C)\n",
        "        x = tok_emb + pos_emb  # (B x T x C)\n",
        "        x = self.blocks(x)  # (B x T x C)\n",
        "        x = self.ln_f(x)  # (B x T x C)\n",
        "        logits = self.lm_head(x)  # (B x T x V)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]  # Indices we'll use for predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]  # (B, V)\n",
        "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "Dk4X56AlIhPQ"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Without Training"
      ],
      "metadata": {
        "id": "P5uZd0K_THmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(n_blocks=6, num_heads=6)\n",
        "model = model.to(device)\n",
        "tokenizer.decode(model.generate(torch.zeros((1, 1), dtype=torch.long, device=device), 1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "GJSDfGZHIzra",
        "outputId": "b1c607e9-347e-4de1-d0a3-de528df9777c"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n½t ûÔ\\'mv4ã$% éÈ!ïû?—:Pºçç–4MwöùmVwí%DmÔî/&ÛãÍ\\nc*v(r¿è”Í\"»èEÇQÇìõÈ-p»qÊvFG!RhC_;üFËªïl–ÛÍm9’3W2ùdìûêw–\\'nSÉ!ÍdìÚuêDõªã21â°+V´?\"â\"1fKLömÜ½öF§+-púçt=Uúd+F2oëºZª’WWeÓîoI4Pëum8ï(MxJxht;o*«œPHO)“ùJ$Tz)çYä!½ãzÛ9ôVê«1âEËï)*(dÈá°úûù/Aë§?Ë;`“;’_6Jë4èUñ…Deáígº¡üäpÊgSG´xmgw¿+QñÕ=â«qí,5LqòdyM5GY5ñî¡A)66vVèJ:’“äèpöÛ5ù.QnììòS¡è‘vè‘59H’RB2.ÃfèçA!worò1eèyAîì«7úêr7”fœGè´ÉOh\\n9;hlèÜ?`½_Gñ¿5eóñé”Í\\'?ûâ¿―sÈQkaéu?‘ÀQõ´X»Ã-N`Oâ–§\\n=$ExjwîúvÀIkâY’¿CÓ-0JÛõ’1bEÂ“FCAÍòîep’”óJVó?N2Wí–«Ae\\'2…JLfâ88wuJdîç…P(Zi3°ÜÔH…z,J§ûäPt1Qd´sZ§Â(1=i\\nTÛ5òB+=1é+àOwO´´M\"mS¿ZkòL9=óAîhwBík??Xààk¡L:ÇÁq?―TJïìuÕÓ2–é.3nJaYÁìÜ?úî”*1à6j:Ízúfáü5Ww2!ÛÂHj‘lœ´J`4ÛyHÇkM;UVMP–ôJ+.OÂ45uÈEVªuª+–Ê2H¿C4îPÊiwùjÇO,î:òì¿õõáÈót5l«6h14Bõó5cÀ‘é§fay=P.çE\\ne¡’Jî:ÂñBHY‘ëKfâmöîzòòœJòO5èn?(LÉ*RœWtn&=_ã:%»è+!Z`ÛûzR…´uq(ÓqÚGO5Qap(wAn–6unM=‘Hvç6hnªUI¡HÂú)ôâ¡Íwò2è’öU=Q?=ÇìïÀY¿´RõÂ2:ë`çë–né½*;eúw\"sE?Êk°áWg–j¡aUY¿jHs´à$«œw)\\nÊÀ1‘’kùGóP%õœHÜ1fs―2Ó6JoV;ª1ñny_e?sõÃa´smõwaDd&TÍ«‘–ªóù8pWEÛpw;Ó¿y%―\\n½¿Vªò/èq1,4‘ëèaÂdìhûâ8néÓF\"“wfG4MY‘ylòó¿âñ+ltÍ5W:ò;nìºÂ´c\\'ÃºFi´iéì0úÜN\\n’10!uÇ¡?6ã'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ChZNQSu0TNoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_batch(split, batch_size):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,), device=device)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]).long()\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).long()\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(batch_size):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters, device=device)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_random_batch(split, batch_size)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "QnoBIwRGI8BF"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "eval_iters = 200\n",
        "\n",
        "lr = 3e-4\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "x_batch, y_batch = get_random_batch(train_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "5fQBREw8Vcs5"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMbQ22olHHh-",
        "outputId": "af113ad4-c73f-4312-bf3f-48539dcf5d8a"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17.938319 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss(batch_size)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_random_batch('train', batch_size)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(tokenizer.decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pBTu2UxHAoO",
        "outputId": "049cb888-35fc-462c-9b15-639196c8e7b8"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 5.1193, val loss 5.1240\n",
            "step 100: train loss 2.4613, val loss 2.4715\n",
            "step 200: train loss 2.4176, val loss 2.4274\n",
            "step 300: train loss 2.3480, val loss 2.3673\n",
            "step 400: train loss 2.2241, val loss 2.2544\n",
            "step 500: train loss 2.1138, val loss 2.1473\n",
            "step 600: train loss 2.0158, val loss 2.0581\n",
            "step 700: train loss 1.9369, val loss 1.9804\n",
            "step 800: train loss 1.8651, val loss 1.9126\n",
            "step 900: train loss 1.8062, val loss 1.8626\n",
            "step 1000: train loss 1.7549, val loss 1.8142\n",
            "step 1100: train loss 1.7155, val loss 1.7718\n",
            "step 1200: train loss 1.6769, val loss 1.7382\n",
            "step 1300: train loss 1.6474, val loss 1.7137\n",
            "step 1400: train loss 1.6152, val loss 1.6825\n",
            "step 1500: train loss 1.5867, val loss 1.6528\n",
            "step 1600: train loss 1.5662, val loss 1.6318\n",
            "step 1700: train loss 1.5388, val loss 1.6058\n",
            "step 1800: train loss 1.5251, val loss 1.5974\n",
            "step 1900: train loss 1.5049, val loss 1.5764\n",
            "step 2000: train loss 1.4887, val loss 1.5573\n",
            "step 2100: train loss 1.4722, val loss 1.5441\n",
            "step 2200: train loss 1.4608, val loss 1.5278\n",
            "step 2300: train loss 1.4512, val loss 1.5209\n",
            "step 2400: train loss 1.4392, val loss 1.5138\n",
            "step 2500: train loss 1.4264, val loss 1.5024\n",
            "step 2600: train loss 1.4171, val loss 1.4961\n",
            "step 2700: train loss 1.4055, val loss 1.4852\n",
            "step 2800: train loss 1.4019, val loss 1.4797\n",
            "step 2900: train loss 1.3962, val loss 1.4756\n",
            "step 3000: train loss 1.3867, val loss 1.4664\n",
            "step 3100: train loss 1.3801, val loss 1.4610\n",
            "step 3200: train loss 1.3715, val loss 1.4513\n",
            "step 3300: train loss 1.3661, val loss 1.4448\n",
            "step 3400: train loss 1.3592, val loss 1.4460\n",
            "step 3500: train loss 1.3521, val loss 1.4329\n",
            "step 3600: train loss 1.3505, val loss 1.4389\n",
            "step 3700: train loss 1.3474, val loss 1.4283\n",
            "step 3800: train loss 1.3403, val loss 1.4283\n",
            "step 3900: train loss 1.3359, val loss 1.4213\n",
            "step 4000: train loss 1.3281, val loss 1.4087\n",
            "step 4100: train loss 1.3250, val loss 1.4097\n",
            "step 4200: train loss 1.3220, val loss 1.4069\n",
            "step 4300: train loss 1.3193, val loss 1.4049\n",
            "step 4400: train loss 1.3147, val loss 1.4023\n",
            "step 4500: train loss 1.3144, val loss 1.4033\n",
            "step 4600: train loss 1.3049, val loss 1.3941\n",
            "step 4700: train loss 1.3031, val loss 1.3943\n",
            "step 4800: train loss 1.2969, val loss 1.3958\n",
            "step 4900: train loss 1.2954, val loss 1.3884\n",
            "step 4999: train loss 1.2934, val loss 1.3853\n",
            "\n",
            "toda a sprema do pude loque.\n",
            "Ele está a alguntar de sala e esta perna completa sombra ao outro, e\n",
            "é outro a origem; lembra-os para o terreno. Daí o que seja o nosso tempo à\n",
            "trocorrido Fluxin 186. De Tijucar a imprensa, raronha, e perdê-lo,\n",
            "a boca olhou no cautor de Sr. Bumbirg. o poeta, ali com um talverez;\n",
            "respondeu consreito, com as, como o Sr. Beugito, na vossa Jers, levando a ferida.\n",
            "Entre a pestinção das Avertinas da caba, no Rio de Jorge Maricó ou filho, flume, subia provoado\n",
            "outra pagado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VW2En7cjXQwc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}